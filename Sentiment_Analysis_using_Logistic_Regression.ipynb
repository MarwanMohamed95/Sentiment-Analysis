{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this Project we will be implementing logistic regression for sentiment analysis on tweets. Given a tweet, we will decide if it has a positive sentiment or a negative one. Specifically we will:\n",
        "\n",
        "- extract features for logistic regression given some text\n",
        "- Implement logistic regression from scratch\n",
        "- Apply logistic regression on a natural language processing task\n",
        "- Test using logistic regression\n",
        "- Perform error analysis\n",
        "\n",
        "We will be using a data set of tweets."
      ],
      "metadata": {
        "id": "emIByZHdfVPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Libraries"
      ],
      "metadata": {
        "id": "96RX8HZcAoX-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlwPWVLxFZhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa57c8f6-f94c-4759-db84-6e2d66ab46c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# run this cell to import nltk\n",
        "import nltk\n",
        "from os import getcwd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples \n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# downloading tweeter samples and stopwords\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Preparing the Data"
      ],
      "metadata": {
        "id": "REO4fZWqAtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "ivwnPe7dNH9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set) \n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg \n",
        "test_x = test_pos + test_neg"
      ],
      "metadata": {
        "id": "fl9xINzRNUOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "metadata": {
        "id": "UwVeR-b4NW--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape train and test sets\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynD_u9JTNaIc",
        "outputId": "d436b5d6-ecf7-4643-fb80-8fb4a9e66d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_y.shape = (8000, 1)\n",
            "test_y.shape = (2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function process_tweet() tokenizes the tweet into individual words, removes stop words and applies stemming."
      ],
      "metadata": {
        "id": "59_8xJ95kd2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# processing tweet\n",
        "def process_tweet(tweet):\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ],
      "metadata": {
        "id": "HgGooah0M6_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the function below\n",
        "print('This is an example of a positive tweet: \\n', train_x[0])\n",
        "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk4KsT_3k6he",
        "outputId": "d58133e2-66e6-4347-ee50-f66211afde5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of a positive tweet: \n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for building frequency table \n",
        "\n",
        "- The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0). The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label."
      ],
      "metadata": {
        "id": "pkjOyyTekSFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freqs(tweets, ys):\n",
        "\n",
        "    # Convert np array to list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs"
      ],
      "metadata": {
        "id": "3kIrAIUqNMxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "# check the output\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRx85c3oNdOA",
        "outputId": "9c00b9fa-5f58-42d7-ee9f-091f84c0f15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(freqs) = 11338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression"
      ],
      "metadata": {
        "id": "xCDM9z86lCB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z): \n",
        "    \n",
        "    # calculate the sigmoid of z\n",
        "    h = 1/(1+np.exp(-z))\n",
        "    \n",
        "    return h"
      ],
      "metadata": {
        "id": "iJrCtI1kNllE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    \n",
        "    # m is the number of rows in matrix x\n",
        "    m = len(x)\n",
        "    \n",
        "    for i in range(0, num_iters):\n",
        "\n",
        "        # get z, the dot product of x and theta\n",
        "        z = np.dot(x,theta)\n",
        "        \n",
        "        # get the sigmoid of z\n",
        "        h = sigmoid(z)\n",
        "        \n",
        "        # calculate the cost function\n",
        "        J = -((y.T@np.log(h)) + ((1-y).T)@np.log(1-h))/(m)\n",
        "\n",
        "        # update the weights theta\n",
        "        theta = theta - (alpha/m)*(x.T@(h-y))\n",
        "        \n",
        "    J = float(J)\n",
        "    return J, theta"
      ],
      "metadata": {
        "id": "1uYCc4KqNteY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting features"
      ],
      "metadata": {
        "id": "WZB2E_j-l93A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
        "    \n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "    \n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3)) \n",
        "    \n",
        "    #bias term is set to 1\n",
        "    x[0,0] = 1     \n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "        \n",
        "        # increment the word count for the positive label 1\n",
        "        x[0,1] += freqs.get((word,1.0),0)\n",
        "        \n",
        "        # increment the word count for the negative label 0\n",
        "        x[0,2] += freqs.get((word,0.0),0)\n",
        "        \n",
        "    assert(x.shape == (1, 3))\n",
        "    return x"
      ],
      "metadata": {
        "id": "m8yWmJtcN5sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collecting the features 'x' for all tweets and stacking them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y"
      ],
      "metadata": {
        "id": "pYSv1_Uxi_Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ],
      "metadata": {
        "id": "nau5An42mXpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUJ_O39NOHhb",
        "outputId": "990b3924-53b3-4671-943a-c65654c2824e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.24215474.\n",
            "The resulting vector of weights is [7e-08, 0.00052391, -0.00055517]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "0d0cI4tmmEKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "\n",
        "    # extracting the features of the tweet and storing it into x\n",
        "    x = extract_features(tweet, freqs)\n",
        "    \n",
        "    # making the prediction using x and theta\n",
        "    y_pred = sigmoid(np.dot(x,theta))\n",
        "        \n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "6TyO-eQVOPA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tweet = 'I am learning :)'\n",
        "predict_tweet(my_tweet, freqs, theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I2VScCmOXo7",
        "outputId": "d77fdc2a-16cc-4874-e891-31f8e681e543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.81636912]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "- Testing the model against the test set.\n",
        "\n",
        "- If the prediction is > 0.5, set the model's classification y_hat to 1, otherwise set the model's classification y_hat to 0."
      ],
      "metadata": {
        "id": "8A1L29F2mLxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
        "    \n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "    \n",
        "    for tweet in test_x:\n",
        "        # geting the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1.0)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0.0)\n",
        "\n",
        "    # computing accuracy\n",
        "    accuracy = np.sum(np.array(y_hat).reshape(-1,1) == test_y)/len(test_y)\n",
        "    \n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "QUBR_rY6Oci0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = compute_accuracy(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rErYn2XFOiDi",
        "outputId": "62618282-b666-4c87-fcc2-f4bfec635881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.9950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
        "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else: \n",
        "    print('Negative sentiment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEJmIITGOqxb",
        "outputId": "8b38d0ea-812a-4efb-e058-09e397b12e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
            "[[0.48139084]]\n",
            "Negative sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x8dR8TdhjsA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}